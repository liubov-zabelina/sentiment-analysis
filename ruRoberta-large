import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import Dataset
from collections import Counter

# 1. Загрузка и подготовка датасета
df = pd.read_excel("Obschiy_dataset.xlsx")
df.columns = ["comment", "sentiment"]
df = df.dropna()
df["sentiment"] = df["sentiment"].astype(int)

#Перекодировка: -1 → 0, 0 → 1, 1 → 2
label_mapping = {-1: 0, 0: 1, 1: 2}
inv_label_mapping = {0: -1, 1: 0, 2: 1}
df["sentiment"] = df["sentiment"].map(label_mapping)

#Оверсэмплинг
counts = df["sentiment"].value_counts()
max_count = counts.max()
df_balanced = pd.concat([
    df[df["sentiment"] == label].sample(max_count, replace=True, random_state=115)
    for label in counts.index
]).sample(frac=1, random_state=115).reset_index(drop=True)

#2. Токенизация и датасеты
model_name = "sberbank-ai/ruRoberta-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)

train_df, eval_df = train_test_split(df_balanced, test_size=0.2, stratify=df_balanced["sentiment"], random_state=115)
train_ds = Dataset.from_pandas(train_df.rename(columns={"comment": "text", "sentiment": "label"}))
eval_ds = Dataset.from_pandas(eval_df.rename(columns={"comment": "text", "sentiment": "label"}))

def tokenize_fn(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=128)

train_tokenized = train_ds.map(tokenize_fn, batched=True)
eval_tokenized = eval_ds.map(tokenize_fn, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

#3. Аргументы обучения
training_args = TrainingArguments(
    output_dir="./results_rurberta",
    do_train=True,
    do_eval=True,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    save_total_limit=1,
    load_best_model_at_end=False,
    fp16=True,
    report_to="none"
)

#4. Метрики
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="macro")
    }

#5. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=eval_tokenized,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

#6. Обучение
print("\n Fine-tuning ruRoberta-large (Colab, GPU, без evaluation_strategy)...")
trainer.train()

#7. Оценка
preds = trainer.predict(eval_tokenized)
y_pred = np.argmax(preds.predictions, axis=1)
y_true = np.array(eval_ds["label"])

y_pred_decoded = np.vectorize(inv_label_mapping.get)(y_pred)
y_true_decoded = np.vectorize(inv_label_mapping.get)(y_true)

acc = accuracy_score(y_true_decoded, y_pred_decoded)
f1 = f1_score(y_true_decoded, y_pred_decoded, average="macro")
cm = confusion_matrix(y_true_decoded, y_pred_decoded, labels=[-1, 0, 1])

print("\n Classification report (ruRoberta-large, Colab):")
print(classification_report(y_true_decoded, y_pred_decoded, digits=3))

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=[-1, 0, 1], yticklabels=[-1, 0, 1])
plt.title("Confusion Matrix: ruRoberta-large (3-class)")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.tight_layout()
plt.show()

print(f"\n Accuracy: {acc:.4f}\n Macro F1-score: {f1:.4f}")
